{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "02b56a83-7f35-4c4d-952a-eaa4ba7ecf84",
   "metadata": {},
   "source": [
    "# COMPARACIÓN DEL Nº DE COPIA DE LOS GENES COMUNES\n",
    "Este script es parte del 3_copy_num, pero lo separo pq va a ser mucho código de golpe, muy distinto, y me ayuda tener las cosas cada una por su cuenta\n",
    "OBJETIVO: comprobar si hay diferencias significativas en el nº de copias de ARG y MGE INDIVIDUALES, COMPARTIDOS entre plásticos y control. De TODOS, ZONA DE MUESTREO A ZONA DE MUESTREO (¡necesariamente tengo que trabajar con los datos chorizeados del Ardley y con Ion + Uruguay juntos!)\n",
    "\n",
    "Ojo que hay dos comparaciones distintas aquí a considerar:\n",
    "1. Comparar el NCN de ARG/MGE considerando TODAS las muestras (= volver a meter 0s)\n",
    "2. Comparar el NCN de ARG/MGE considerando SÓLO cuando están presentes (= no meter 0s de vuelta)\n",
    "\n",
    "De la primera manera, el hecho de que pueda haber más detecciones per se en plástico afectará. De la segunda, la pregunta es más específica y las conclusiones menos extrapolables, pero es necesario complementar la primera respuesta con la segunda.\n",
    "\n",
    "---\n",
    "\n",
    "Y sea cual sea el caso, lo primero siempre va a ser ver qué genes son comunes en plástico y medio circundante muestra a muestra. Ese código, ese listado lo puedo reciclar, y usarlo por igual en ambas aproximaciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fdd39d6c-ac28-413b-8443-1ddbfe9b7b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "import numpy as np\n",
    "import scikit_posthocs as sp\n",
    "import matplotlib\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "687c20ba-af12-41ea-baed-496085fc7c65",
   "metadata": {},
   "source": [
    "## COMPARACIÓN DE NCN A NIVEL DE GEN INDIVIDUAL CONSIDERANDO TODOS LOS RESULTADOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "621db254-5545-450f-bfd4-e804d81754db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>place</th>\n",
       "      <th>Assay</th>\n",
       "      <th>type_g</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ardley</td>\n",
       "      <td>ARR-3</td>\n",
       "      <td>['plastic' 'control']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ardley</td>\n",
       "      <td>Aac6-Aph2</td>\n",
       "      <td>['plastic' 'control']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ardley</td>\n",
       "      <td>Arr2</td>\n",
       "      <td>['plastic' 'control']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ardley</td>\n",
       "      <td>IS1133</td>\n",
       "      <td>['plastic' 'control']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ardley</td>\n",
       "      <td>IS1247</td>\n",
       "      <td>['plastic' 'control']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>422</th>\n",
       "      <td>uru</td>\n",
       "      <td>mphA</td>\n",
       "      <td>['control' 'plastic']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>423</th>\n",
       "      <td>uru</td>\n",
       "      <td>oprD</td>\n",
       "      <td>['plastic' 'control']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>424</th>\n",
       "      <td>uru</td>\n",
       "      <td>orf37-IS26</td>\n",
       "      <td>['plastic' 'control']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>426</th>\n",
       "      <td>uru</td>\n",
       "      <td>qepA_1_2</td>\n",
       "      <td>['plastic' 'control']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>432</th>\n",
       "      <td>uru</td>\n",
       "      <td>trb-C</td>\n",
       "      <td>['plastic' 'control']</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>261 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      place       Assay                 type_g\n",
       "0    ardley       ARR-3  ['plastic' 'control']\n",
       "1    ardley   Aac6-Aph2  ['plastic' 'control']\n",
       "3    ardley        Arr2  ['plastic' 'control']\n",
       "6    ardley      IS1133  ['plastic' 'control']\n",
       "7    ardley      IS1247  ['plastic' 'control']\n",
       "..      ...         ...                    ...\n",
       "422     uru        mphA  ['control' 'plastic']\n",
       "423     uru        oprD  ['plastic' 'control']\n",
       "424     uru  orf37-IS26  ['plastic' 'control']\n",
       "426     uru    qepA_1_2  ['plastic' 'control']\n",
       "432     uru       trb-C  ['plastic' 'control']\n",
       "\n",
       "[261 rows x 3 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data = pd.read_csv(\"../data/resistome_data/clean_data/ab_data_all_fixed.csv\", index_col = 0) # CHORICEO\n",
    "all_data.dropna(inplace = True)\n",
    "all_data = all_data[(all_data[\"antib\"] != \"16S\")]\n",
    "all_data = all_data[[\"Assay\", \"antib\", \"sample\", \"place\", \"type_f\", \"type_g\", \"or_seq\", \"rel_ab\"]] #place viene a ser la estrella del show\n",
    "all_data = all_data.loc[~all_data[\"place\"].isin([0])]\n",
    "all_data.loc[all_data[\"sample\"] == 13, \"rel_ab\"] = all_data.loc[all_data[\"sample\"] == 13][\"rel_ab\"] / 3\n",
    "all_data.loc[all_data[\"sample\"] == 14, \"rel_ab\"] = all_data.loc[all_data[\"sample\"] == 14][\"rel_ab\"] / 3\n",
    "#all_data = all_data[(all_data.Assay != \"aadA7\") & (all_data.sample != 5)] #esto también es tremendamente importante\n",
    "\n",
    "common_amps = all_data[[\"Assay\", \"place\", \"type_g\"]].groupby([\"place\", \"Assay\"])[\"type_g\"].unique()\n",
    "del all_data\n",
    "common_amps = common_amps.reset_index()\n",
    "common_amps[\"type_g\"] = common_amps[\"type_g\"].astype(\"str\") #hacky as shit\n",
    "common_amps = common_amps.loc[(common_amps[\"type_g\"] == \"['plastic' 'control']\") | (common_amps[\"type_g\"] == \"['control' 'plastic']\")]\n",
    "common_amps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96745bc7-9b2e-449e-a68e-79d084daf56a",
   "metadata": {},
   "source": [
    "Ahora procedo a quedarme sólo con las partes específicas en las que me interesa estudiar la diferencia. De entrada, separaré lagos y Ardley por la gran influencia de este sobre todos los resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "761d2361-de2e-4441-9169-3d5cd44f1639",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are  10314 ARGvMGE in all the data.  6876  just in lake samples.  246 are shared among plastics and the environment (with all replicates)\n",
      "(which correspond to  20  unique sequences)\n",
      "There are  10314 ARGvMGE in all the data.  3438  just in ardley samples.  2115 are shared among plastics and the environment (with all replicates)\n",
      "(which correspond to  235  unique sequences)\n"
     ]
    }
   ],
   "source": [
    "# Voy a 0s, endo me quedo todos los ceros habidos y por haber\n",
    "all_data = pd.read_csv(\"../data/resistome_data/clean_data/ab_data_all_fixed.csv\", index_col = 0) # CHORICEO\n",
    "all_data = all_data[[\"Assay\", \"sample\", \"place\", \"type_f\", \"or_seq\", \"rel_ab\"]]\n",
    "all_data[\"rel_ab\"].fillna(0, inplace = True)\n",
    "all_data[\"n_log2\"] = np.log2(all_data[\"rel_ab\"] + 1)\n",
    "#print(all_data.shape)\n",
    "#all_data.loc[(all_data[\"place\"] == \"ion\") |(all_data[\"place\"] == \"uru\"), \"place\"] = \"lake\"\n",
    "lake_data = all_data.loc[(all_data[\"place\"] == \"ion\") |(all_data[\"place\"] == \"uru\")] #sólo los lagos\n",
    "temp = lake_data.shape[0]\n",
    "lake_data = lake_data.loc[((lake_data[\"place\"] == \"ion\") & (lake_data[\"Assay\"].isin(list(common_amps.loc[common_amps[\"place\"] == \"ion\"][\"Assay\"].unique()))))\n",
    "                         |\n",
    "                         ((lake_data[\"place\"] == \"uru\") & (lake_data[\"Assay\"].isin(list(common_amps.loc[common_amps[\"place\"] == \"uru\"][\"Assay\"].unique()))))]\n",
    "print(\"There are \", all_data.shape[0], \"ARGvMGE in all the data. \", temp, \" just in lake samples. \", lake_data.shape[0], \"are shared among plastics and the environment (with all replicates)\")\n",
    "print(\"(which correspond to \", len(lake_data[\"Assay\"].unique()), \" unique sequences)\")\n",
    "lake_data.loc[(lake_data[\"place\"] == \"ion\") |(lake_data[\"place\"] == \"uru\"), \"place\"] = \"lake\"\n",
    "ardley_data = all_data.loc[all_data[\"place\"] == \"ardley\"]\n",
    "temp = ardley_data.shape[0]\n",
    "ardley_data = ardley_data.loc[ardley_data[\"Assay\"].isin(list(common_amps.loc[common_amps[\"place\"] == \"ardley\"][\"Assay\"].unique()))]\n",
    "print(\"There are \", all_data.shape[0], \"ARGvMGE in all the data. \", temp, \" just in ardley samples. \", ardley_data.shape[0], \"are shared among plastics and the environment (with all replicates)\")\n",
    "print(\"(which correspond to \", len(ardley_data[\"Assay\"].unique()), \" unique sequences)\")\n",
    "del all_data\n",
    "del temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2a9e3c89-78a1-4cf1-9f9e-de81996b091e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for dataset in [lake_data, ardley_data]:\n",
    "    for seq in dataset[\"Assay\"].unique():\n",
    "        wd = dataset.loc[dataset[\"Assay\"] == seq] #working data: nuestro Assay de interés en cada momento\n",
    "        var_lst_ln = [group[\"n_log2\"] for name, group in wd.groupby([\"or_seq\", \"type_f\"])]\n",
    "        nam_lst = [name for name, group in wd.groupby([\"or_seq\", \"type_f\"])]\n",
    "        levene = stats.levene(*var_lst_ln)\n",
    "        if levene[1] >= 0.05:\n",
    "            kruskal = stats.kruskal(*var_lst_ln)\n",
    "            if kruskal[1] < 0.05: \n",
    "                count += 1\n",
    "                #print(\"Results for sequence \", seq, \"in \", wd[\"place\"].unique())\n",
    "                #print(kruskal)\n",
    "                #post_lolo = sp.posthoc_conover(var_lst_ln, p_adjust = \"fdr_bh\")\n",
    "                #post_lolo.rename(columns = {original_num:new_name for (original_num, new_name) in zip(post_lolo.columns, nam_lst)},\n",
    "                #                 index = {original_num:new_name for (original_num, new_name) in zip(post_lolo.index, nam_lst)},\n",
    "                #                 inplace = True)\n",
    "                #post_lolo = post_lolo[post_lolo < 0.05]\n",
    "                #print(post_lolo)\n",
    "\n",
    "print(count)            \n",
    "# Y ahora ver solo lo interesante\n",
    "#post_lolo = post_lolo[post_lolo < 0.05]\n",
    "#esto enfrenta a control (filas) vs plástico (columnas)\n",
    "#post_lolo_control = post_lolo.iloc[0:post_lolo.shape[0]:2,1:(post_lolo.shape[1]+1):2]\n",
    "#y esto saca sólo la diagonal\n",
    "#post_lolo_diag = pd.Series(np.diag(post_lolo_control), index=[post_lolo_control.index, post_lolo_control.columns])\n",
    "#post_lolo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e54d5cf-03e3-482a-b4e8-d5e7a675e53d",
   "metadata": {},
   "source": [
    "Ahora voy a comparar por pares plástico vs control sin más remordimiento. Esto va a suponer dos problemas inmediatos:\n",
    "1. Si hago un test estadístico con un valor = NaN, el resultado no va a ser fiable (va a ser NaN), puesto que estoy metiendo infinitos\n",
    "2. Paralelamente, si no tengo tres réplicas, el test no corre. Asi que no puedo usar todo el dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b52d9008-e6a8-42f3-a263-6d8a775e4509",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def statistic_mean(x, y, axis):\n",
    "    return np.mean(x, axis=axis) - np.mean(y, axis=axis)\n",
    "def get_difs(groupby_data, var = \"rel_ab\", detailed = False, alternative = \"two-sided\", significant = True, grouping_var = \"type_g\"):\n",
    "    \"\"\"\n",
    "    A highly hardcoded function that I'm defining only because it will make the rest of the notebook cleaner. Designed to take a multi-indexed groupby object\n",
    "    and perform some basic analysys on it: normality, homocedasticity and the corresponding test of means' difference. It probably incurs in Type I error, as it compares multiples\n",
    "    pairs of control-sample samples (they are NOT paired samples, mind you)\n",
    "    \n",
    "    - groupby_data: the groupby object that contains the data to analyze. As it is programmed, it MUST contain a column with a descriptive, binary value: control and study samples\n",
    "    - var: name of the column whose values we want to compare\n",
    "    - detailed: change it to True to get a detailed printing of all the tests being conducted\n",
    "    - alternative: which altenative hypothesis to feed to the paired tests. I0m running scipy tests, so it works with the same values (less, two-sided, greater)\n",
    "    - significant: if True, keep just those with pval < 0.05; if False, keep everything\n",
    "    \"\"\"\n",
    "    result_dict = {}\n",
    "    for name, group in groupby_data:\n",
    "        name = str(name).upper()\n",
    "        pl_data = group.loc[group[grouping_var] == \"plastic\"][var]\n",
    "        con_data = group.loc[group[grouping_var] == \"control\"][var]\n",
    "        \n",
    "        # Tests de normalidad\n",
    "        norm_plas = stats.shapiro(pl_data)\n",
    "        norm_con = stats.shapiro(con_data)\n",
    "        \n",
    "        # Tests de homocedasticidad\n",
    "        if (norm_plas[1] > 0.05) and (norm_con[1] > 0.05):\n",
    "            norm = True\n",
    "            homo = stats.bartlett(pl_data, con_data)\n",
    "        else: \n",
    "            norm = False\n",
    "            homo = stats.levene(pl_data, con_data)\n",
    "            \n",
    "        # Test de igualdad de distribuciones\n",
    "        if (homo[1] > 0.05) and (norm == True): \n",
    "            result = stats.ttest_ind(a = pl_data, b = con_data, alternative = alternative)\n",
    "        elif (homo[1] > 0.05): #homocedásticos no normales\n",
    "            result = stats.mannwhitneyu(x = pl_data, y = con_data, alternative = alternative)\n",
    "        elif (homo[1] < 0.05) and (norm == True): #normales no homocedásticos\n",
    "            result = stats.ttest_ind(a = pl_data, b = con_data, alternative = alternative, equal_var = False)\n",
    "            result = (\"Welch!\", result[1], result[0])\n",
    "        else: #todo mal, permuta\n",
    "            result = stats.permutation_test(data = (pl_data, con_data), statistic =  statistic_mean, \n",
    "                                            vectorized = True, n_resamples = 20000,\n",
    "                                            alternative = alternative, permutation_type = \"independent\")\n",
    "            result = (\"Permutation\", result.pvalue, result.statistic) #ugly hack!\n",
    "        if significant == True:\n",
    "            if result[1] < 0.05: result_dict[name] = result\n",
    "        else: \n",
    "            result_dict[name] = result\n",
    "            \n",
    "            \n",
    "            \n",
    "        if detailed == True:\n",
    "            print(\"##########\", name, \"#########\")\n",
    "            print(\"### PLASTIC ###\")\n",
    "            print(norm_plas)\n",
    "            print(\"### CONTROL ###\")\n",
    "            print(norm_con)\n",
    "            print(\"### BOTH ###\")\n",
    "            print(homo)\n",
    "            print(\"### DIFFERENCES ###\")\n",
    "            print(result)\n",
    "            print(\"\")\n",
    "    return result_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0725d371-a857-4b6d-90e6-35fa1b77ff07",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_difs(groupby_data, var = \"rel_ab\", detailed = False, alternative = \"two-sided\", significant = True, grouping_var = \"type_g\"):\n",
    "    \"\"\"\n",
    "    A highly hardcoded function that I'm defining only because it will make the rest of the notebook cleaner. Designed to take a multi-indexed groupby object\n",
    "    and perform some basic analysys on it: normality, homocedasticity and the corresponding test of means' difference. It probably incurs in Type I error, as it compares multiples\n",
    "    pairs of control-sample samples (they are NOT paired samples, mind you)\n",
    "    \n",
    "    - groupby_data: the groupby object that contains the data to analyze. As it is programmed, it MUST contain a column with a descriptive, binary value: control and study samples\n",
    "    - var: name of the column whose values we want to compare\n",
    "    - detailed: change it to True to get a detailed printing of all the tests being conducted\n",
    "    - alternative: which altenative hypothesis to feed to the paired tests. I0m running scipy tests, so it works with the same values (less, two-sided, greater)\n",
    "    - significant: if True, keep just those with pval < 0.05; if False, keep everything\n",
    "    \"\"\"\n",
    "    result_dict = {}\n",
    "    for name, group in groupby_data:\n",
    "        name = str(name)#.upper()\n",
    "        pl_data = group.loc[group[grouping_var] == \"plastic\"][var]\n",
    "        con_data = group.loc[group[grouping_var] == \"control\"][var]\n",
    "        \n",
    "        # Tests de normalidad\n",
    "        norm_plas = stats.shapiro(pl_data)\n",
    "        norm_con = stats.shapiro(con_data)\n",
    "        \n",
    "        # Tests de homocedasticidad\n",
    "        if (norm_plas[1] > 0.05) and (norm_con[1] > 0.05):\n",
    "            norm = True\n",
    "            homo = stats.bartlett(pl_data, con_data)\n",
    "        else: \n",
    "            norm = False\n",
    "            homo = stats.levene(pl_data, con_data)\n",
    "            \n",
    "        # Test de igualdad de distribuciones\n",
    "        if (homo[1] > 0.05) and (norm == True): \n",
    "            result = stats.ttest_ind(a = pl_data, b = con_data, alternative = alternative)\n",
    "        elif (homo[1] > 0.05): #homocedásticos no normales\n",
    "            result = stats.mannwhitneyu(x = pl_data, y = con_data, alternative = alternative)\n",
    "        elif (homo[1] < 0.05) and (norm == True): #normales no homocedásticos\n",
    "            result = stats.ttest_ind(a = pl_data, b = con_data, alternative = alternative, equal_var = False)\n",
    "            result = (\"Welch!\", result[1], result[0])\n",
    "        else: #todo mal, permuta\n",
    "            result = stats.permutation_test(data = (pl_data, con_data), statistic =  statistic_mean, \n",
    "                                            vectorized = True, n_resamples = 20000,\n",
    "                                            alternative = alternative, permutation_type = \"independent\")\n",
    "            result = (\"Permutation\", result.pvalue, result.statistic) #ugly hack!\n",
    "        if significant == True:\n",
    "            if result[1] < 0.05: result_dict[name] = result\n",
    "        else: \n",
    "            result_dict[name] = result\n",
    "            \n",
    "            \n",
    "            \n",
    "        if detailed == True:\n",
    "            print(\"##########\", name, \"#########\")\n",
    "            print(\"### PLASTIC ###\")\n",
    "            print(norm_plas)\n",
    "            print(\"### CONTROL ###\")\n",
    "            print(norm_con)\n",
    "            print(\"### BOTH ###\")\n",
    "            print(homo)\n",
    "            print(\"### DIFFERENCES ###\")\n",
    "            print(result)\n",
    "            print(\"\")\n",
    "    return result_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "af81e464-02ca-46a0-939a-439e752e213c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are  10287 ARGvMGE in all the data.  6858  just in lake samples.  168 are shared among plastics and the environment (with all replicates)\n",
      "(which correspond to  14  unique sequences)\n",
      "There are  10287 ARGvMGE in all the data.  3429  just in ardley samples.  2106 are shared among plastics and the environment (with all replicates)\n",
      "(which correspond to  234  unique sequences)\n"
     ]
    }
   ],
   "source": [
    "all_data = pd.read_csv(\"../data/resistome_data/clean_data/ab_data_all_fixed.csv\", index_col = 0)\n",
    "all_data = all_data[[\"Assay\", \"sample\", \"place\", \"type_g\", \"or_seq\", \"rel_ab\"]]\n",
    "all_data[\"rel_ab\"].fillna(0, inplace = True)\n",
    "all_data[\"n_log2\"] = np.log2(all_data[\"rel_ab\"] + 1)  #esto evita meter 0s\n",
    "all_data.loc[all_data[\"sample\"] == 13, \"rel_ab\"] = all_data.loc[all_data[\"sample\"] == 13][\"rel_ab\"] / 3\n",
    "all_data.loc[all_data[\"sample\"] == 14, \"rel_ab\"] = all_data.loc[all_data[\"sample\"] == 14][\"rel_ab\"] / 3\n",
    "all_data = all_data[(all_data.Assay != \"aadA7\") & (all_data.sample != 5)] #esto también es tremendamente importante\n",
    "#all_data.dropna(inplace = True)\n",
    "\n",
    "#print(all_data.shape)\n",
    "#all_data.loc[(all_data[\"place\"] == \"ion\") |(all_data[\"place\"] == \"uru\"), \"place\"] = \"lake\"\n",
    "lake_data = all_data.loc[(all_data[\"place\"] == \"ion\") |(all_data[\"place\"] == \"uru\")] #sólo los lagos\n",
    "temp = lake_data.shape[0]\n",
    "lake_data = lake_data.loc[((lake_data[\"place\"] == \"uru\") & (lake_data[\"Assay\"].isin(list(common_amps.loc[common_amps[\"place\"] == \"uru\"][\"Assay\"].unique()))))]\n",
    "print(\"There are \", all_data.shape[0], \"ARGvMGE in all the data. \", temp, \" just in lake samples. \", lake_data.shape[0], \"are shared among plastics and the environment (with all replicates)\")\n",
    "print(\"(which correspond to \", len(lake_data[\"Assay\"].unique()), \" unique sequences)\")\n",
    "lake_data.loc[(lake_data[\"place\"] == \"ion\") |(lake_data[\"place\"] == \"uru\"), \"place\"] = \"lake\"\n",
    "ardley_data = all_data.loc[all_data[\"place\"] == \"ardley\"]\n",
    "temp = ardley_data.shape[0]\n",
    "ardley_data = ardley_data.loc[ardley_data[\"Assay\"].isin(list(common_amps.loc[common_amps[\"place\"] == \"ardley\"][\"Assay\"].unique()))]\n",
    "print(\"There are \", all_data.shape[0], \"ARGvMGE in all the data. \", temp, \" just in ardley samples. \", ardley_data.shape[0], \"are shared among plastics and the environment (with all replicates)\")\n",
    "print(\"(which correspond to \", len(ardley_data[\"Assay\"].unique()), \" unique sequences)\")\n",
    "del all_data\n",
    "del temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "96bc002c-4191-409f-9377-69762839b5b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "general_log2_difs = {}\n",
    "general_log2_difs[\"lake_difs\"] = get_difs(lake_data.groupby(\"Assay\"), var = \"n_log2\",\n",
    "                                           detailed = False, alternative = \"two-sided\", significant = True)\n",
    "general_log2_difs[\"ardley_difs\"] = get_difs(ardley_data.groupby(\"Assay\"), var = \"n_log2\",\n",
    "                                           detailed = False, alternative = \"two-sided\", significant = True)\n",
    "#general_log2_difs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f0c4898-7672-4000-8651-b9c84d1e4987",
   "metadata": {},
   "source": [
    "Ahora tengo un listado de todos las secuencias que tienen una diferencia estadísticamente significativa entre control y plástico, pero no sé si todos todos tienen mayor NCN en plásticos y no hay alguno travieso en control. Asi que voy a hacer algo estúpido: loopear por las llaves del diccionario de resultados, sacar todos los datos de esa secuencia para n_log2 y restar la suma de todos los plásticos a la suma de todos los controles. Si saliera negativo, es mayor en lagos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "936b43e8-224b-424e-aa8d-ed30d85602c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for seq in general_log2_difs[\"lake_difs\"].keys():\n",
    "    interest_data = lake_data.loc[lake_data[\"Assay\"] == seq]\n",
    "    dif = interest_data.loc[interest_data[\"type_g\"] == \"plastic\"][\"n_log2\"].sum() - interest_data.loc[interest_data[\"type_g\"] == \"control\"][\"n_log2\"].sum()\n",
    "    if dif > 0:\n",
    "        general_log2_difs[\"lake_difs\"][seq] = list(general_log2_difs[\"lake_difs\"][seq])\n",
    "        general_log2_difs[\"lake_difs\"][seq].append(\"plastic\")\n",
    "    else:\n",
    "        general_log2_difs[\"lake_difs\"][seq] = list(general_log2_difs[\"lake_difs\"][seq])\n",
    "        general_log2_difs[\"lake_difs\"][seq].append(\"control\")\n",
    "\n",
    "for seq in general_log2_difs[\"ardley_difs\"].keys():\n",
    "    interest_data = ardley_data.loc[ardley_data[\"Assay\"] == seq]\n",
    "    dif = interest_data.loc[interest_data[\"type_g\"] == \"plastic\"][\"n_log2\"].sum() - interest_data.loc[interest_data[\"type_g\"] == \"control\"][\"n_log2\"].sum()\n",
    "    if dif > 0:\n",
    "        general_log2_difs[\"ardley_difs\"][seq] = list(general_log2_difs[\"ardley_difs\"][seq])\n",
    "        general_log2_difs[\"ardley_difs\"][seq].append(\"plastic\")\n",
    "    else:\n",
    "        general_log2_difs[\"ardley_difs\"][seq] = list(general_log2_difs[\"ardley_difs\"][seq])\n",
    "        general_log2_difs[\"ardley_difs\"][seq].append(\"control\")\n",
    "        \n",
    "import json\n",
    "\n",
    "with open(\"../data/resistome_data/metadata/shared_different_seqs_per_place.json\", \"w\") as fp:\n",
    "    json.dump(general_log2_difs, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4561d926-6f52-440a-90e5-355e36845b34",
   "metadata": {},
   "source": [
    "Se da la mala suerte de que para aquellos resultados de T-test normales o mannwhitneys estoy perdiendo info de de dónde vienen (de qué test vienen esos resultados). Pero bueno, un poco lo que hay. Si viera a ojo que todo son plásticos puedo simplemente quedarme con los resultados del paso anterior y dejar ese print en el código como prueba."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f5d02d3-5c58-454c-a52c-e62ebc8da763",
   "metadata": {},
   "source": [
    "NOTA: es el caso, asi que vuelvo a correr los resultados normales y me quedo esos"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
